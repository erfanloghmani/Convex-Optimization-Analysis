# Analyzing some Convex Optimization Algorithms and Enhancement methods

this repo contains two notebooks that investigate on convex optimization algorithms and enhancement methods. This analyze was done to find a good project for Convex Optimization course.

## Second order methods
This notebook compares:
- Gradient Decent
- Newton Method
- Natural Gradient Method

algorithms on a simple classification problem with artificially generated data. It also checks the effect of reparametrization on each algorithm.

## SVRG
This notebook compares:
- Stochastic Gradient Decent
- Mini-batch Stochastic Gradient Decent
- SVRG (Stochastic Variance Reduced Gradient)

algorithms on learning multi-class classification on MNIST dataset. Algorithms are evaluated with their loss decay, accuracy, and variance.
