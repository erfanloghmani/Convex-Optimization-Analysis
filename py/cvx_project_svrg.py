# -*- coding: utf-8 -*-
"""CVX project: SVRG

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AiCVsJLRfxRY6pu0aQd9T6lXssvD-uoY
"""

import torch
from torch import nn as nn
from torch.nn import functional as F
from torch import optim

# Commented out IPython magic to ensure Python compatibility.
import torchvision
from torchvision import transforms as transforms

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import random

import copy

# %matplotlib inline

torch.manual_seed(0)

"""## Load Data"""

M = 6000

# torchvision.datasets.MNIST outputs a set of PIL images
# We transform them to tensors
transform = transforms.ToTensor()

# Load and transform data
trainset = torchvision.datasets.MNIST('/tmp', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=M, shuffle=True, num_workers=3)

testset = torchvision.datasets.MNIST('/tmp', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False, num_workers=3)

device = torch.device('cpu')

train_data_X, train_data_Y = next(iter(trainloader))
train_data_X = train_data_X.view(train_data_X.shape[0], -1).to(device)
train_data_Y = train_data_Y.to(device)

train_data_X.shape

test_data_X, test_data_Y = next(iter(testloader))
test_data_X = test_data_X.view(test_data_X.shape[0], -1).to(device)
test_data_Y = test_data_Y.to(device)

"""### Label distribution"""

sns.distplot(train_data_Y.cpu().numpy())
plt.show()

sns.distplot(test_data_Y.cpu().numpy())
plt.show()

"""# SGD"""

def SGD(rp=1e-4, lr=1e-2, batch_size=1, epoch_count=100, log_every=12000):
  torch.manual_seed(0)
  full_loss_list = []
  epoch_variance_list = []
  accuracy_list = []
  W = torch.randn(train_data_X.shape[1], 10, requires_grad=True, device=device)
  b = torch.randn(10, requires_grad=True, device=device)
  loss = nn.CrossEntropyLoss()

  step_W_history = torch.zeros((M // batch_size, 28*28, 10))
  step_b_history = torch.zeros((M // batch_size, 10))

  for e in range(M // batch_size * epoch_count):
    if W.grad is not None:
      W.grad.zero_()
      b.grad.zero_()

    idx = random.randrange(M // batch_size)
    data_X = train_data_X[idx * batch_size:(idx + 1) * batch_size]
    data_Y = train_data_Y[idx * batch_size:(idx + 1) * batch_size]
    out = data_X.matmul(W) + b
    l = loss(out, data_Y) + 1./ 2 * rp * (torch.trace(W.T.matmul(W)) + b.dot(b.T))
    l.backward()

    if (e + 1) % log_every == 0:
      print(e, l.item())
      print(out, out.argmax(1), train_data_Y[idx * batch_size:(idx + 1) * batch_size])

      sns.lineplot(range(len(full_loss_list)), full_loss_list)
      plt.show()
      sns.lineplot(range(len(epoch_variance_list)), epoch_variance_list)
      plt.show()
      sns.lineplot(range(len(accuracy_list)), accuracy_list)
      plt.show()
    with torch.no_grad():
      step_W = lr * W.grad
      step_b = lr * b.grad
      W -= step_W
      b -= step_b

      step_W_history[e % (M // batch_size)] = step_W.clone().detach()
      step_b_history[e % (M // batch_size)] = step_b.clone().detach()

      if (e + 1) % (M // batch_size) == 0:
        out = train_data_X.matmul(W) + b
        l = loss(out, train_data_Y) + 1. / 2 * rp * (torch.trace(W.T.matmul(W)) + b.dot(b.T))
        full_loss_list.append(l.item())

        mvar = torch.sum(torch.var(step_W_history, axis=0)) + torch.sum(torch.var(step_b_history, axis=0))
        epoch_variance_list.append(mvar.item())

        out = test_data_X.matmul(W) + b
        acc = torch.mean((out.argmax(dim=1) == test_data_Y).float())
        accuracy_list.append(acc.item())
  return W.clone().detach(), b.clone().detach(), full_loss_list, epoch_variance_list, accuracy_list

W_sgd_1, b_sgd_1, full_loss_list_sgd_1, epoch_variance_list_sgd_1, accuracy_list_sgd_1 = SGD(lr=0.01, log_every=M*10)

W_sgd_2, b_sgd_2, full_loss_list_sgd_2, epoch_variance_list_sgd_2, accuracy_list_sgd_2 = SGD(lr=0.05, log_every=M*10)

"""# SGD minibatch"""

W_mb_1, b_mb_1, full_loss_list_mb_1, epoch_variance_list_mb_1, accuracy_list_mb_1 = SGD(lr=0.04, log_every=M*10//4, batch_size=4)

W_mb_2, b_mb_2, full_loss_list_mb_2, epoch_variance_list_mb_2, accuracy_list_mb_2 = SGD(lr=0.2, log_every=M*10//4, batch_size=4)

"""# SVRG"""

def SVRG(rp=1e-4, lr=0.01, outer_loop_count=100, m=6000, log_every_outer_loop=1):
  torch.manual_seed(0)
  full_loss_list = []
  inner_loop_variance_list = []
  accuracy_list = []

  W = torch.randn(train_data_X.shape[1], 10, requires_grad=True, device=device)
  b = torch.randn(10, requires_grad=True, device=device)
  loss = nn.CrossEntropyLoss()

  step_W_history = torch.zeros((m, 28*28, 10))
  step_b_history = torch.zeros((m, 10))

  W_s = torch.zeros((outer_loop_count + 1, 28*28, 10), device=device)
  b_s = torch.zeros((outer_loop_count + 1, 10), device=device)

  W_s[0] = W.clone().detach()
  b_s[0] = b.clone().detach()

  for s in range(1, outer_loop_count + 1):
    W_hat = W_s[s - 1].clone().detach().requires_grad_(True)
    b_hat = b_s[s - 1].clone().detach().requires_grad_(True)

    if W_hat.grad is not None:
      W_hat.grad.zero_()
      b_hat.grad.zero_()

    out = train_data_X.matmul(W_hat) + b_hat
    l = loss(out, train_data_Y) + 1./ 2 * rp * (torch.trace(W_hat.T.matmul(W_hat)) + b_hat.dot(b_hat.T))
    l.backward()

    mu_hat_W = W_hat.grad.clone().detach()
    mu_hat_b = b_hat.grad.clone().detach()

    for t in range(m):
      if W.grad is not None:
        W.grad.zero_()
        b.grad.zero_()
      if W_hat.grad is not None:
        W_hat.grad.zero_()
        b_hat.grad.zero_()

      idx = random.randrange(M)
      data_X = train_data_X[idx:idx + 1]
      data_Y = train_data_Y[idx:idx + 1]
      out = data_X.matmul(W) + b
      l = loss(out, data_Y) + 1./ 2 * rp * (torch.trace(W.T.matmul(W)) + b.dot(b.T))
      l.backward()

      out = data_X.matmul(W_hat) + b_hat
      l = loss(out, data_Y) + 1./ 2 * rp * (torch.trace(W_hat.T.matmul(W_hat)) + b_hat.dot(b_hat.T))
      l.backward()

      with torch.no_grad():
        step_W = lr * (W.grad - W_hat.grad + mu_hat_W)
        step_b = lr * (b.grad - b_hat.grad + mu_hat_b)
        W -= step_W
        b -= step_b
        step_W_history[t] = step_W.clone().detach()
        step_b_history[t] = step_b.clone().detach()

    with torch.no_grad():
      W_s[s] = W.clone().detach()
      b_s[s] = b.clone().detach()

      out = train_data_X.matmul(W) + b
      l = loss(out, train_data_Y) + 1. / 2 * rp * (torch.trace(W.T.matmul(W)) + b.dot(b.T))
      full_loss_list.append(l.item())

      mvar = torch.sum(torch.var(step_W_history, axis=0)) + torch.sum(torch.var(step_b_history, axis=0))
      inner_loop_variance_list.append(mvar.item())

      out = test_data_X.matmul(W) + b
      acc = torch.mean((out.argmax(dim=1) == test_data_Y).float())
      accuracy_list.append(acc.item())

    if s % log_every_outer_loop == 0:
      sns.lineplot(range(len(full_loss_list)), full_loss_list)
      plt.show()
      sns.lineplot(range(len(inner_loop_variance_list)), inner_loop_variance_list)
      plt.show()
      sns.lineplot(range(len(accuracy_list)), accuracy_list)
      plt.show()
  return W.clone().detach(), b.clone().detach(), full_loss_list, inner_loop_variance_list, accuracy_list

W_svrg_1, b_svrg_1, full_loss_list_svrg_1, inner_loop_variance_list_svrg_1, accuracy_list_svrg_1 = SVRG(lr=0.01, m=M, log_every_outer_loop=10)

W_svrg_2, b_svrg_2, full_loss_list_svrg_2, inner_loop_variance_list_svrg_2, accuracy_list_svrg_2 = SVRG(lr=0.05, m=M, log_every_outer_loop=10)

"""## Plot results

### Training loss
"""

loss_df = pd.DataFrame({
    'SGD-0.01': full_loss_list_sgd_1,
    'SGD-0.05': full_loss_list_sgd_2,
    'Minibatch-0.04': full_loss_list_mb_1,
    'Minibatch-0.20': full_loss_list_mb_2,
    'SVRG-0.01': full_loss_list_svrg_1,
    'SVRG-0.05': full_loss_list_svrg_2,
})

loss_df_melted = pd.melt(loss_df.reset_index(), id_vars='index', value_vars=loss_df.columns, var_name='Method')

g = sns.lineplot(x='index', y='value', hue='Method', data=loss_df_melted)
g.set(xlabel='epoch', ylabel='loss')
g.set(title='Training loss over epochs')
plt.show()

g = sns.lineplot(x='index', y='value', hue='Method', data=loss_df_melted)
g.set(xlabel='epoch', ylabel='loss')
g.set(ylim=(None, 0.8))
g.set(title='Training loss over epochs zoomed in')
plt.show()

"""### Epoch step variance"""

var_df = pd.DataFrame({
    'SGD-0.01': epoch_variance_list_sgd_1,
    'SGD-0.05': epoch_variance_list_sgd_2,
    'Minibatch-0.04': epoch_variance_list_mb_1,
    'Minibatch-0.20': epoch_variance_list_mb_2,
    'SVRG-0.01': inner_loop_variance_list_svrg_1,
    'SVRG-0.05': inner_loop_variance_list_svrg_2,
})

var_df_melted = pd.melt(var_df.reset_index(), id_vars='index', value_vars=var_df.columns, var_name='Method')

g = sns.lineplot(x='index', y='value', hue='Method', data=var_df_melted)
g.set(xlabel='epoch', ylabel='variance')
g.set(title='Step variance over epochs')
g.set(yscale='log')
plt.show()

"""### Test accuracy"""

acc_df = pd.DataFrame({
    'SGD-0.01': accuracy_list_sgd_1,
    'SGD-0.05': accuracy_list_sgd_2,
    'Minibatch-0.04': accuracy_list_mb_1,
    'Minibatch-0.20': accuracy_list_mb_2,
    'SVRG-0.01': accuracy_list_svrg_1,
    'SVRG-0.05': accuracy_list_svrg_2,
})

acc_df_melted = pd.melt(acc_df.reset_index(), id_vars='index', value_vars=acc_df.columns, var_name='Method')

g = sns.lineplot(x='index', y='value', hue='Method', data=acc_df_melted)
g.set(xlabel='epoch', ylabel='accuracy')
g.set(title='Test accuracy over epochs')
plt.show()

g = sns.lineplot(x='index', y='value', hue='Method', data=acc_df_melted)
g.set(xlabel='epoch', ylabel='accuracy')
g.set(title='Test accuracy over epochs zoomed in')
g.set(ylim=(0.85, None))
plt.show()
